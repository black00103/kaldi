nohup: ignoring input
./run.sh: Lexicon Preparation
Dictionary preparation succeeded
./run.sh: Data Preparation
utils/fix_data_dir.sh: file data/train/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/train/wav.scp is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 3119 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
cp data/train/text data/local/train/text for language model training
utils/fix_data_dir.sh: file data/test/utt2spk is not in sorted order or not unique, sorting it
utils/fix_data_dir.sh: file data/test/wav.scp is not in sorted order or not unique, sorting it
fix_data_dir.sh: kept all 346 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
Data preparation completed.
./run.sh: Phone Sets, questions, L compilation Preparation
utils/prepare_lang.sh --position-dependent-phones false data/local/dict <SIL> data/local/lang data/lang
Checking data/local/dict/silence_phones.txt ...
--> reading data/local/dict/silence_phones.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/silence_phones.txt is OK

Checking data/local/dict/optional_silence.txt ...
--> reading data/local/dict/optional_silence.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/optional_silence.txt is OK

Checking data/local/dict/nonsilence_phones.txt ...
--> reading data/local/dict/nonsilence_phones.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/nonsilence_phones.txt is OK

Checking disjoint: silence_phones.txt, nonsilence_phones.txt
--> disjoint property is OK.

Checking data/local/dict/lexicon.txt
--> reading data/local/dict/lexicon.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/lexicon.txt is OK

Checking data/local/dict/extra_questions.txt ...
--> reading data/local/dict/extra_questions.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/extra_questions.txt is OK
--> SUCCESS [validating dictionary directory data/local/dict]

**Creating data/local/dict/lexiconp.txt from data/local/dict/lexicon.txt
fstaddselfloops data/lang/phones/wdisambig_phones.int data/lang/phones/wdisambig_words.int 
prepare_lang.sh: validating output directory
utils/validate_lang.pl data/lang
Checking existence of separator file
separator file data/lang/subword_separator.txt is empty or does not exist, deal in word case.
Checking data/lang/phones.txt ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/lang/phones.txt is OK

Checking words.txt: #0 ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/lang/words.txt is OK

Checking disjoint: silence.txt, nonsilence.txt, disambig.txt ...
--> silence.txt and nonsilence.txt are disjoint
--> silence.txt and disambig.txt are disjoint
--> disambig.txt and nonsilence.txt are disjoint
--> disjoint property is OK

Checking sumation: silence.txt, nonsilence.txt, disambig.txt ...
--> found no unexplainable phones in phones.txt

Checking data/lang/phones/context_indep.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/phones/context_indep.txt
--> data/lang/phones/context_indep.int corresponds to data/lang/phones/context_indep.txt
--> data/lang/phones/context_indep.csl corresponds to data/lang/phones/context_indep.txt
--> data/lang/phones/context_indep.{txt, int, csl} are OK

Checking data/lang/phones/nonsilence.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 91 entry/entries in data/lang/phones/nonsilence.txt
--> data/lang/phones/nonsilence.int corresponds to data/lang/phones/nonsilence.txt
--> data/lang/phones/nonsilence.csl corresponds to data/lang/phones/nonsilence.txt
--> data/lang/phones/nonsilence.{txt, int, csl} are OK

Checking data/lang/phones/silence.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/phones/silence.txt
--> data/lang/phones/silence.int corresponds to data/lang/phones/silence.txt
--> data/lang/phones/silence.csl corresponds to data/lang/phones/silence.txt
--> data/lang/phones/silence.{txt, int, csl} are OK

Checking data/lang/phones/optional_silence.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.int corresponds to data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.csl corresponds to data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.{txt, int, csl} are OK

Checking data/lang/phones/disambig.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 2 entry/entries in data/lang/phones/disambig.txt
--> data/lang/phones/disambig.int corresponds to data/lang/phones/disambig.txt
--> data/lang/phones/disambig.csl corresponds to data/lang/phones/disambig.txt
--> data/lang/phones/disambig.{txt, int, csl} are OK

Checking data/lang/phones/roots.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 92 entry/entries in data/lang/phones/roots.txt
--> data/lang/phones/roots.int corresponds to data/lang/phones/roots.txt
--> data/lang/phones/roots.{txt, int} are OK

Checking data/lang/phones/sets.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 92 entry/entries in data/lang/phones/sets.txt
--> data/lang/phones/sets.int corresponds to data/lang/phones/sets.txt
--> data/lang/phones/sets.{txt, int} are OK

Checking data/lang/phones/extra_questions.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 2 entry/entries in data/lang/phones/extra_questions.txt
--> data/lang/phones/extra_questions.int corresponds to data/lang/phones/extra_questions.txt
--> data/lang/phones/extra_questions.{txt, int} are OK

Checking optional_silence.txt ...
--> reading data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.txt is OK

Checking disambiguation symbols: #0 and #1
--> data/lang/phones/disambig.txt has "#0" and "#1"
--> data/lang/phones/disambig.txt is OK

Checking topo ...

Checking word-level disambiguation symbols...
--> data/lang/phones/wdisambig.txt exists (newer prepare_lang.sh)
Checking data/lang/oov.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/oov.txt
--> data/lang/oov.int corresponds to data/lang/oov.txt
--> data/lang/oov.{txt, int} are OK

--> data/lang/L.fst is olabel sorted
--> data/lang/L_disambig.fst is olabel sorted
--> SUCCESS [validating lang directory data/lang]
./run.sh: LM training
Getting raw N-gram counts
discount_ngrams: for n-gram order 1, D=0.000000, tau=0.000000 phi=1.000000
discount_ngrams: for n-gram order 2, D=0.000000, tau=0.000000 phi=1.000000
discount_ngrams: for n-gram order 3, D=1.000000, tau=0.000000 phi=1.000000
Iteration 1/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.675000 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.675000 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.825000 phi=2.000000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.900000 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.900000 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.100000 phi=2.000000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=1.215000 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=1.215000 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.485000 phi=2.000000
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 130.989920
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.779247

real	0m0.089s
user	0m0.119s
sys	0m0.000s
Perplexity over 179.000000 words is 131.370181
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 133.205888

real	0m0.088s
user	0m0.114s
sys	0m0.004s
Perplexity over 179.000000 words is 131.094405
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.924690

real	0m0.093s
user	0m0.114s
sys	0m0.005s
Projected perplexity change from setting alpha=-0.463878120728513 is 131.094405->130.961711945659, reduction of 0.13269305434099
Alpha value on iter 1 is -0.463878120728513
Iteration 2/6 of optimizing discounting parameters
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.442301 phi=2.000000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.589734 phi=2.000000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.796141 phi=2.000000
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 130.376983
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.189472
Perplexity over 179.000000 words is 130.544993
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.360775

real	0m0.090s
user	0m0.119s
sys	0m0.000s

real	0m0.089s
user	0m0.119s
sys	0m0.000s
Perplexity over 179.000000 words is 130.784737
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.605221

real	0m0.089s
user	0m0.105s
sys	0m0.013s
optimize_alpha.pl: alpha=-15.7020860927563 is too negative, limiting it to -0.5
Projected perplexity change from setting alpha=-0.5 is 130.544993->130.211669428571, reduction of 0.333323571428593
Alpha value on iter 2 is -0.5
Iteration 3/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=1.750000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.000000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.350000
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 130.213178
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.022457

real	0m0.088s
user	0m0.115s
sys	0m0.004s
Perplexity over 179.000000 words is 130.198110
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 132.007094

real	0m0.088s
user	0m0.120s
sys	0m0.000s
Perplexity over 179.000000 words is 130.153891
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 131.962009

real	0m0.094s
user	0m0.117s
sys	0m0.007s
optimize_alpha.pl: objective function is not convex; returning alpha=0.7
Projected perplexity change from setting alpha=0.7 is 130.198110->130.082694233333, reduction of 0.115415766666786
Alpha value on iter 3 is 0.7
Iteration 4/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=1.080000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 140.394101
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 141.562841

real	0m0.065s
user	0m0.085s
sys	0m0.000s
Perplexity over 179.000000 words is 127.146653
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.293940

real	0m0.089s
user	0m0.116s
sys	0m0.004s
Perplexity over 179.000000 words is 130.118905
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 131.926337

real	0m0.089s
user	0m0.107s
sys	0m0.013s
Projected perplexity change from setting alpha=-0.329176810526953 is 130.118905->126.964135204544, reduction of 3.15476979545615
Alpha value on iter 4 is -0.329176810526953
Iteration 5/6 of optimizing discounting parameters
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.361882 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.651388 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 127.082253
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.333067

real	0m0.089s
user	0m0.111s
sys	0m0.010s
Perplexity over 179.000000 words is 127.287226
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.566416

real	0m0.090s
user	0m0.119s
sys	0m0.000s
Perplexity over 179.000000 words is 126.976132
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.186590

real	0m0.088s
user	0m0.118s
sys	0m0.000s
Projected perplexity change from setting alpha=0.351045613499538 is 127.082253->126.9761310585, reduction of 0.106121941500263
Alpha value on iter 5 is 0.351045613499538
Iteration 6/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.651893 phi=1.750000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.651893 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.651893 phi=2.350000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
interpolate_ngrams: 804 words in wordslist
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 126.871534
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.054311

real	0m0.088s
user	0m0.119s
sys	0m0.000s
Perplexity over 179.000000 words is 126.977285
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.187770

real	0m0.088s
user	0m0.115s
sys	0m0.003s
Perplexity over 179.000000 words is 126.904059
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.154318

real	0m0.089s
user	0m0.117s
sys	0m0.002s
optimize_alpha.pl: objective function is not convex; returning alpha=-0.5
Projected perplexity change from setting alpha=-0.5 is 126.977285->126.634070261905, reduction of 0.343214738095242
Alpha value on iter 6 is -0.5
Final config is:
D=0.6 tau=0.482509691344338 phi=2
D=0.536658551578438 tau=0.651892601961784 phi=1.5
D=0 tau=0.294867033599318 phi=2.7
Discounting N-grams.
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.482510 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.536659, tau=0.651893 phi=1.500000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.294867 phi=2.700000
Computing final perplexity
Building ARPA LM (perplexity computation is in background)
interpolate_ngrams: 804 words in wordslist
interpolate_ngrams: 804 words in wordslist
Perplexity over 179.000000 words is 127.100995
Perplexity over 178.000000 words (excluding 1.000000 OOVs) is 129.227083
127.100995
Done training LM of type 3gram-mincount
./run.sh: G compilation, check LG composition
Converting 'data/local/lm/3gram-mincount/lm_unpruned.gz' to FST
arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G.fst 
LOG (arpa2fst[5.5.690~1-9b4dc]:Read():arpa-file-parser.cc:94) Reading \data\ section.
LOG (arpa2fst[5.5.690~1-9b4dc]:Read():arpa-file-parser.cc:149) Reading \1-grams: section.
LOG (arpa2fst[5.5.690~1-9b4dc]:Read():arpa-file-parser.cc:149) Reading \2-grams: section.
LOG (arpa2fst[5.5.690~1-9b4dc]:Read():arpa-file-parser.cc:149) Reading \3-grams: section.
LOG (arpa2fst[5.5.690~1-9b4dc]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 20418 to 4036
fstisstochastic data/lang_test/G.fst 
-9.47196e-07 -0.393371
Succeeded in formatting LM: 'data/local/lm/3gram-mincount/lm_unpruned.gz'
./run.sh: making mfccs
steps/make_mfcc_pitch.sh --cmd run.pl --nj 20 data/train exp/make_mfcc/train mfcc
steps/make_mfcc_pitch.sh: moving data/train/feats.scp to data/train/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/train
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_mfcc_pitch.sh: Succeeded creating MFCC and pitch features for train
steps/compute_cmvn_stats.sh data/train exp/make_mfcc/train mfcc
Succeeded creating CMVN stats for train
fix_data_dir.sh: kept all 3119 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
steps/make_mfcc_pitch.sh --cmd run.pl --nj 20 data/test exp/make_mfcc/test mfcc
steps/make_mfcc_pitch.sh: moving data/test/feats.scp to data/test/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/test
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_mfcc_pitch.sh: Succeeded creating MFCC and pitch features for test
steps/compute_cmvn_stats.sh data/test exp/make_mfcc/test mfcc
Succeeded creating CMVN stats for test
fix_data_dir.sh: kept all 346 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
./run.sh: train mono model
./run.sh: make training subsets
feat-to-len scp:data/train/feats.scp ark,t:data/train_mono/tmp.len 
utils/subset_data_dir.sh: reducing #utt from 3119 to 3000
steps/train_mono.sh --boost-silence 1.25 --cmd run.pl --nj 20 data/train_mono data/lang exp/mono
steps/train_mono.sh: Initializing monophone system.
steps/train_mono.sh: Compiling training graphs
steps/train_mono.sh: Aligning data equally (pass 0)
steps/train_mono.sh: Pass 1
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 2
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 3
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 4
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 5
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 6
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 7
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 8
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 9
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 10
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 11
steps/train_mono.sh: Pass 12
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 13
steps/train_mono.sh: Pass 14
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 15
steps/train_mono.sh: Pass 16
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 17
steps/train_mono.sh: Pass 18
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 19
steps/train_mono.sh: Pass 20
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 21
steps/train_mono.sh: Pass 22
steps/train_mono.sh: Pass 23
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 24
steps/train_mono.sh: Pass 25
steps/train_mono.sh: Pass 26
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 27
steps/train_mono.sh: Pass 28
steps/train_mono.sh: Pass 29
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 30
steps/train_mono.sh: Pass 31
steps/train_mono.sh: Pass 32
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 33
steps/train_mono.sh: Pass 34
steps/train_mono.sh: Pass 35
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 36
steps/train_mono.sh: Pass 37
steps/train_mono.sh: Pass 38
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 39
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/mono
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 57.2721179625% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/mono/log/analyze_alignments.log
1 warnings in exp/mono/log/analyze_alignments.log
1470 warnings in exp/mono/log/align.*.*.log
642 warnings in exp/mono/log/update.*.log
449 warnings in exp/mono/log/acc.*.*.log
exp/mono: nj=20 align prob=-85.78 over 3.60h [retry=2.5%, fail=0.4%] states=278 gauss=1001
steps/train_mono.sh: Done training monophone system in exp/mono
steps/align_si.sh --boost-silence 1.25 --cmd run.pl --nj 20 data/train data/lang exp/mono exp/mono_ali
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/mono, putting alignments in exp/mono_ali
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/mono_ali
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 56.8754034861% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/mono_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
./run.sh: train tri1 model
steps/train_deltas.sh --boost-silence 1.25 --cmd run.pl 2500 20000 data/train data/lang exp/mono_ali exp/tri1
tree-info exp/mono/tree 
tree-info exp/mono/tree 
fstpushspecial 
fsttablecompose data/lang_test/L_disambig.fst data/lang_test/G.fst 
fstdeterminizestar --use-log=true 
fstminimizeencoded 
steps/train_deltas.sh: accumulating tree stats
fstisstochastic data/lang_test/tmp/LG.fst 
-0.0397074 -0.0400977
[info]: LG not stochastic.
fstcomposecontext --context-size=1 --central-position=0 --read-disambig-syms=data/lang_test/phones/disambig.int --write-disambig-syms=data/lang_test/tmp/disambig_ilabels_1_0.int data/lang_test/tmp/ilabels_1_0.22649 data/lang_test/tmp/LG.fst 
fstisstochastic data/lang_test/tmp/CLG_1_0.fst 
-0.0397074 -0.0400977
[info]: CLG not stochastic.
make-h-transducer --disambig-syms-out=exp/mono/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_1_0 exp/mono/tree exp/mono/final.mdl 
fstrmepslocal 
fsttablecompose exp/mono/graph/Ha.fst data/lang_test/tmp/CLG_1_0.fst 
fstminimizeencoded 
fstdeterminizestar --use-log=true 
fstrmsymbols exp/mono/graph/disambig_tid.int 
fstisstochastic exp/mono/graph/HCLGa.fst 
0 -0.0800781
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/mono/final.mdl exp/mono/graph/HCLGa.fst 
steps/decode.sh --cmd run.pl --config conf/decode.config --nj 20 exp/mono/graph data/test exp/mono/decode_test
steps/train_deltas.sh: getting questions for tree-building, via clustering
decode.sh: feature type is delta
steps/train_deltas.sh: building the tree
WARNING (gmm-init-model[5.5.690~1-9b4dc]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 19 with no stats; corresponding phone list: 20 
** The warnings above about 'no stats' generally mean you have phones **
** (or groups of phones) in your phone set that had no corresponding data. **
** You should probably figure out whether something went wrong, **
** or whether your data just doesn't happen to have examples of those **
** phones. **
steps/train_deltas.sh: converting alignments from exp/mono_ali to use current tree
steps/train_deltas.sh: compiling graphs of transcripts
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono/graph exp/mono/decode_test
steps/train_deltas.sh: training pass 1
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 52.8901734104% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/mono/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,5,22) and mean=9.7
steps/diagnostic/analyze_lats.sh: see stats in exp/mono/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/mono/graph exp/mono/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/mono/graph exp/mono/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
steps/train_deltas.sh: training pass 2
steps/train_deltas.sh: training pass 3
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/mono/graph exp/mono/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/mono/graph exp/mono/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
steps/train_deltas.sh: training pass 4
steps/train_deltas.sh: training pass 5
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/train_deltas.sh: training pass 6
steps/train_deltas.sh: training pass 7
steps/train_deltas.sh: training pass 8
steps/train_deltas.sh: training pass 9
steps/train_deltas.sh: training pass 10
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 11
steps/train_deltas.sh: training pass 12
steps/train_deltas.sh: training pass 13
steps/train_deltas.sh: training pass 14
steps/train_deltas.sh: training pass 15
steps/train_deltas.sh: training pass 16
steps/train_deltas.sh: training pass 17
steps/train_deltas.sh: training pass 18
steps/train_deltas.sh: training pass 19
steps/train_deltas.sh: training pass 20
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 21
steps/train_deltas.sh: training pass 22
steps/train_deltas.sh: training pass 23
steps/train_deltas.sh: training pass 24
steps/train_deltas.sh: training pass 25
steps/train_deltas.sh: training pass 26
steps/train_deltas.sh: training pass 27
steps/train_deltas.sh: training pass 28
steps/train_deltas.sh: training pass 29
steps/train_deltas.sh: training pass 30
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 31
steps/train_deltas.sh: training pass 32
steps/train_deltas.sh: training pass 33
steps/train_deltas.sh: training pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri1
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 65.4738878143% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 42.5525040388% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1/log/analyze_alignments.log
1 warnings in exp/tri1/log/build_tree.log
77 warnings in exp/tri1/log/update.*.log
1 warnings in exp/tri1/log/questions.log
1 warnings in exp/tri1/log/compile_questions.log
578 warnings in exp/tri1/log/acc.*.*.log
2 warnings in exp/tri1/log/analyze_alignments.log
219 warnings in exp/tri1/log/align.*.*.log
50 warnings in exp/tri1/log/init_model.log
exp/tri1: nj=20 align prob=-78.00 over 4.18h [retry=1.6%, fail=0.5%] states=1840 gauss=20049 tree-impr=8.19
steps/train_deltas.sh: Done training system with delta+delta-delta features in exp/tri1
steps/align_si.sh --cmd run.pl --nj 20 data/train data/lang exp/tri1 exp/tri1_ali
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/tri1, putting alignments in exp/tri1_ali
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri1_ali
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 62.3791102515% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 41.338073691% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
./run.sh: train tri2 model
steps/train_deltas.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri1_ali exp/tri2
tree-info exp/tri1/tree 
tree-info exp/tri1/tree 
steps/train_deltas.sh: accumulating tree stats
fstcomposecontext --context-size=3 --central-position=1 --read-disambig-syms=data/lang_test/phones/disambig.int --write-disambig-syms=data/lang_test/tmp/disambig_ilabels_3_1.int data/lang_test/tmp/ilabels_3_1.10294 data/lang_test/tmp/LG.fst 
fstisstochastic data/lang_test/tmp/CLG_3_1.fst 
0 -0.0400977
[info]: CLG not stochastic.
make-h-transducer --disambig-syms-out=exp/tri1/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_3_1 exp/tri1/tree exp/tri1/final.mdl 
fstdeterminizestar --use-log=true 
fsttablecompose exp/tri1/graph/Ha.fst data/lang_test/tmp/CLG_3_1.fst 
fstminimizeencoded 
fstrmepslocal 
fstrmsymbols exp/tri1/graph/disambig_tid.int 
steps/train_deltas.sh: getting questions for tree-building, via clustering
fstisstochastic exp/tri1/graph/HCLGa.fst 
0.000486833 -0.101771
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri1/final.mdl exp/tri1/graph/HCLGa.fst 
steps/decode.sh --cmd run.pl --config conf/decode.config --nj 20 exp/tri1/graph data/test exp/tri1/decode_test
decode.sh: feature type is delta
steps/train_deltas.sh: building the tree
WARNING (gmm-init-model[5.5.690~1-9b4dc]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 19 with no stats; corresponding phone list: 20 
** The warnings above about 'no stats' generally mean you have phones **
** (or groups of phones) in your phone set that had no corresponding data. **
** You should probably figure out whether something went wrong, **
** or whether your data just doesn't happen to have examples of those **
** phones. **
steps/train_deltas.sh: converting alignments from exp/tri1_ali to use current tree
steps/train_deltas.sh: compiling graphs of transcripts
steps/train_deltas.sh: training pass 1
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri1/graph exp/tri1/decode_test
steps/train_deltas.sh: training pass 2
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 60.6936416185% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 43.768115942% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri1/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,5) and mean=2.6
steps/diagnostic/analyze_lats.sh: see stats in exp/tri1/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri1/graph exp/tri1/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri1/graph exp/tri1/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
steps/train_deltas.sh: training pass 3
steps/train_deltas.sh: training pass 4
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri1/graph exp/tri1/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri1/graph exp/tri1/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
steps/train_deltas.sh: training pass 5
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/train_deltas.sh: training pass 6
steps/train_deltas.sh: training pass 7
steps/train_deltas.sh: training pass 8
steps/train_deltas.sh: training pass 9
steps/train_deltas.sh: training pass 10
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 11
steps/train_deltas.sh: training pass 12
steps/train_deltas.sh: training pass 13
steps/train_deltas.sh: training pass 14
steps/train_deltas.sh: training pass 15
steps/train_deltas.sh: training pass 16
steps/train_deltas.sh: training pass 17
steps/train_deltas.sh: training pass 18
steps/train_deltas.sh: training pass 19
steps/train_deltas.sh: training pass 20
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 21
steps/train_deltas.sh: training pass 22
steps/train_deltas.sh: training pass 23
steps/train_deltas.sh: training pass 24
steps/train_deltas.sh: training pass 25
steps/train_deltas.sh: training pass 26
steps/train_deltas.sh: training pass 27
steps/train_deltas.sh: training pass 28
steps/train_deltas.sh: training pass 29
steps/train_deltas.sh: training pass 30
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 31
steps/train_deltas.sh: training pass 32
steps/train_deltas.sh: training pass 33
steps/train_deltas.sh: training pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri2
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 52.7249274428% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 33.8182993857% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri2/log/analyze_alignments.log
1 warnings in exp/tri2/log/build_tree.log
603 warnings in exp/tri2/log/acc.*.*.log
1 warnings in exp/tri2/log/compile_questions.log
67 warnings in exp/tri2/log/update.*.log
46 warnings in exp/tri2/log/init_model.log
2 warnings in exp/tri2/log/analyze_alignments.log
1 warnings in exp/tri2/log/questions.log
222 warnings in exp/tri2/log/align.*.*.log
exp/tri2: nj=20 align prob=-77.93 over 4.18h [retry=1.4%, fail=0.6%] states=1864 gauss=20035 tree-impr=8.78
steps/train_deltas.sh: Done training system with delta+delta-delta features in exp/tri2
steps/align_si.sh --cmd run.pl --nj 20 data/train data/lang exp/tri2 exp/tri2_ali
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/tri2, putting alignments in exp/tri2_ali
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri2_ali
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 51.2415349887% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 32.9130294213% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri2_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
ehB: train tri3 model
steps/train_lda_mllt.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri2_ali exp/tri3a
tree-info exp/tri2/tree 
tree-info exp/tri2/tree 
make-h-transducer --disambig-syms-out=exp/tri2/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_3_1 exp/tri2/tree exp/tri2/final.mdl 
steps/train_lda_mllt.sh: Accumulating LDA statistics.
fstdeterminizestar --use-log=true 
fsttablecompose exp/tri2/graph/Ha.fst data/lang_test/tmp/CLG_3_1.fst 
fstminimizeencoded 
fstrmsymbols exp/tri2/graph/disambig_tid.int 
fstrmepslocal 
steps/train_lda_mllt.sh: Accumulating tree stats
fstisstochastic exp/tri2/graph/HCLGa.fst 
0.000486833 -0.10171
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri2/final.mdl exp/tri2/graph/HCLGa.fst 
steps/decode.sh --cmd run.pl --config conf/decode.config --nj 20 exp/tri2/graph data/test exp/tri2/decode_test
decode.sh: feature type is delta
steps/train_lda_mllt.sh: Getting questions for tree clustering.
steps/train_lda_mllt.sh: Building the tree
steps/train_lda_mllt.sh: Initializing the model
WARNING (gmm-init-model[5.5.690~1-9b4dc]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 19 with no stats; corresponding phone list: 20 
This is a bad warning.
steps/train_lda_mllt.sh: Converting alignments from exp/tri2_ali to use current tree
steps/train_lda_mllt.sh: Compiling graphs of transcripts
Training pass 1
Training pass 2
steps/train_lda_mllt.sh: Estimating MLLT
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri2/graph exp/tri2/decode_test
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 47.1098265896% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 35.3623188406% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri2/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,5) and mean=2.6
steps/diagnostic/analyze_lats.sh: see stats in exp/tri2/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri2/graph exp/tri2/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri2/graph exp/tri2/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
Training pass 3
Training pass 4
steps/train_lda_mllt.sh: Estimating MLLT
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri2/graph exp/tri2/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri2/graph exp/tri2/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
Training pass 5
Training pass 6
steps/train_lda_mllt.sh: Estimating MLLT
Training pass 7
Training pass 8
Training pass 9
Training pass 10
Aligning data
Training pass 11
Training pass 12
steps/train_lda_mllt.sh: Estimating MLLT
Training pass 13
Training pass 14
Training pass 15
Training pass 16
Training pass 17
Training pass 18
Training pass 19
Training pass 20
Aligning data
Training pass 21
Training pass 22
Training pass 23
Training pass 24
Training pass 25
Training pass 26
Training pass 27
Training pass 28
Training pass 29
Training pass 30
Aligning data
Training pass 31
Training pass 32
Training pass 33
Training pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri3a
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 43.4558349452% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 26.6256874798% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri3a/log/analyze_alignments.log
198 warnings in exp/tri3a/log/align.*.*.log
18 warnings in exp/tri3a/log/lda_acc.*.log
587 warnings in exp/tri3a/log/acc.*.*.log
1 warnings in exp/tri3a/log/compile_questions.log
1 warnings in exp/tri3a/log/build_tree.log
68 warnings in exp/tri3a/log/update.*.log
35 warnings in exp/tri3a/log/init_model.log
1 warnings in exp/tri3a/log/questions.log
2 warnings in exp/tri3a/log/analyze_alignments.log
exp/tri3a: nj=20 align prob=-47.02 over 4.18h [retry=1.4%, fail=0.5%] states=1944 gauss=20032 tree-impr=8.02 lda-sum=36.31 mllt:impr,logdet=1.32,2.32
steps/train_lda_mllt.sh: Done training system with LDA+MLLT features in exp/tri3a
./run.sh: train tri4 model
steps/align_fmllr.sh --cmd run.pl --nj 20 data/train data/lang exp/tri3a exp/tri3a_ali
tree-info exp/tri3a/tree 
tree-info exp/tri3a/tree 
make-h-transducer --disambig-syms-out=exp/tri3a/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_3_1 exp/tri3a/tree exp/tri3a/final.mdl 
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
fstminimizeencoded 
fstdeterminizestar --use-log=true 
fstrmepslocal 
fsttablecompose exp/tri3a/graph/Ha.fst data/lang_test/tmp/CLG_3_1.fst 
fstrmsymbols exp/tri3a/graph/disambig_tid.int 
steps/align_fmllr.sh: aligning data in data/train using exp/tri3a/final.mdl and speaker-independent features.
fstisstochastic exp/tri3a/graph/HCLGa.fst 
0.000486833 -0.101701
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri3a/final.mdl exp/tri3a/graph/HCLGa.fst 
steps/decode.sh --cmd run.pl --nj 20 --config conf/decode.config exp/tri3a/graph data/test exp/tri3a/decode_test
decode.sh: feature type is lda
steps/align_fmllr.sh: computing fMLLR transforms
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri3a/graph exp/tri3a/decode_test
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 39.8843930636% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 32.7536231884% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri3a/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,4) and mean=2.4
steps/diagnostic/analyze_lats.sh: see stats in exp/tri3a/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri3a/graph exp/tri3a/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri3a/graph exp/tri3a/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
steps/align_fmllr.sh: doing final alignment.
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri3a/graph exp/tri3a/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri3a/graph exp/tri3a/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri3a_ali
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 41.94068343% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 26.334519573% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri3a_ali/log/analyze_alignments.log
2 warnings in exp/tri3a_ali/log/analyze_alignments.log
2257 warnings in exp/tri3a_ali/log/fmllr.*.log
61 warnings in exp/tri3a_ali/log/align_pass1.*.log
60 warnings in exp/tri3a_ali/log/align_pass2.*.log
steps/train_sat.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri3a_ali exp/tri4a
steps/train_sat.sh: feature type is lda
steps/train_sat.sh: Using transforms from exp/tri3a_ali
steps/train_sat.sh: Accumulating tree stats
steps/train_sat.sh: Getting questions for tree clustering.
steps/train_sat.sh: Building the tree
steps/train_sat.sh: Initializing the model
WARNING (gmm-init-model[5.5.690~1-9b4dc]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 19 with no stats; corresponding phone list: 20 
This is a bad warning.
steps/train_sat.sh: Converting alignments from exp/tri3a_ali to use current tree
steps/train_sat.sh: Compiling graphs of transcripts
Pass 1
Pass 2
Estimating fMLLR transforms
Pass 3
Pass 4
Estimating fMLLR transforms
Pass 5
Pass 6
Estimating fMLLR transforms
Pass 7
Pass 8
Pass 9
Pass 10
Aligning data
Pass 11
Pass 12
Estimating fMLLR transforms
Pass 13
Pass 14
Pass 15
Pass 16
Pass 17
Pass 18
Pass 19
Pass 20
Aligning data
Pass 21
Pass 22
Pass 23
Pass 24
Pass 25
Pass 26
Pass 27
Pass 28
Pass 29
Pass 30
Aligning data
Pass 31
Pass 32
Pass 33
Pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri4a
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 39.5033860045% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 24.6677471637% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri4a/log/analyze_alignments.log
32 warnings in exp/tri4a/log/init_model.log
67 warnings in exp/tri4a/log/update.*.log
1 warnings in exp/tri4a/log/questions.log
1 warnings in exp/tri4a/log/build_tree.log
1 warnings in exp/tri4a/log/compile_questions.log
603 warnings in exp/tri4a/log/acc.*.*.log
2 warnings in exp/tri4a/log/est_alimdl.log
2 warnings in exp/tri4a/log/analyze_alignments.log
195 warnings in exp/tri4a/log/align.*.*.log
9028 warnings in exp/tri4a/log/fmllr.*.*.log
steps/train_sat.sh: Likelihood evolution:
-53.3318 -52.8246 -52.6909 -52.4764 -51.5806 -50.6059 -50.0487 -49.6144 -49.1855 -48.6167 -48.2589 -47.654 -47.3495 -47.1346 -46.9386 -46.7591 -46.5876 -46.4211 -46.2636 -46.0443 -45.864 -45.7324 -45.6061 -45.4852 -45.3727 -45.2619 -45.1539 -45.0462 -44.9411 -44.8125 -44.7241 -44.6935 -44.6741 -44.6613 
exp/tri4a: nj=20 align prob=-47.56 over 4.18h [retry=1.4%, fail=0.6%] states=1968 gauss=20021 fmllr-impr=1.45 over 3.52h tree-impr=10.47
steps/train_sat.sh: done training SAT system in exp/tri4a
steps/align_fmllr.sh --cmd run.pl --nj 20 data/train data/lang exp/tri4a exp/tri4a_ali
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train using exp/tri4a/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri4a_ali
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 38.6326991293% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 24.8217757615% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri4a_ali/log/analyze_alignments.log
62 warnings in exp/tri4a_ali/log/align_pass1.*.log
2247 warnings in exp/tri4a_ali/log/fmllr.*.log
2 warnings in exp/tri4a_ali/log/analyze_alignments.log
61 warnings in exp/tri4a_ali/log/align_pass2.*.log
./run.sh: train tri5 model
steps/train_sat.sh --cmd run.pl 3500 100000 data/train data/lang exp/tri4a_ali exp/tri5a
tree-info exp/tri4a/tree 
tree-info exp/tri4a/tree 
make-h-transducer --disambig-syms-out=exp/tri4a/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_3_1 exp/tri4a/tree exp/tri4a/final.mdl 
steps/train_sat.sh: feature type is lda
steps/train_sat.sh: Using transforms from exp/tri4a_ali
steps/train_sat.sh: Accumulating tree stats
fstrmsymbols exp/tri4a/graph/disambig_tid.int 
fsttablecompose exp/tri4a/graph/Ha.fst data/lang_test/tmp/CLG_3_1.fst 
fstminimizeencoded 
fstrmepslocal 
fstdeterminizestar --use-log=true 
steps/train_sat.sh: Getting questions for tree clustering.
fstisstochastic exp/tri4a/graph/HCLGa.fst 
0.000486833 -0.101972
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri4a/final.mdl exp/tri4a/graph/HCLGa.fst 
steps/decode_fmllr.sh --cmd run.pl --nj 20 --config conf/decode.config exp/tri4a/graph data/test exp/tri4a/decode_test
steps/decode.sh --scoring-opts  --num-threads 1 --skip-scoring false --acwt 0.083333 --nj 20 --cmd run.pl --beam 8.0 --model exp/tri4a/final.alimdl --max-active 2000 exp/tri4a/graph data/test exp/tri4a/decode_test.si
decode.sh: feature type is lda
steps/train_sat.sh: Building the tree
steps/train_sat.sh: Initializing the model
WARNING (gmm-init-model[5.5.690~1-9b4dc]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 19 with no stats; corresponding phone list: 20 
This is a bad warning.
steps/train_sat.sh: Converting alignments from exp/tri4a_ali to use current tree
steps/train_sat.sh: Compiling graphs of transcripts
Pass 1
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri4a/graph exp/tri4a/decode_test.si
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 35.549132948% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 34.8837209302% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri4a/decode_test.si/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,4) and mean=2.3
steps/diagnostic/analyze_lats.sh: see stats in exp/tri4a/decode_test.si/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test.si
steps/score_kaldi.sh --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test.si
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
Pass 2
Estimating fMLLR transforms
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test.si
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test.si
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/decode_fmllr.sh: feature type is lda
steps/decode_fmllr.sh: getting first-pass fMLLR transforms.
steps/decode_fmllr.sh: doing main lattice generation phase
Pass 3
Pass 4
Estimating fMLLR transforms
steps/decode_fmllr.sh: estimating fMLLR transforms a second time.
steps/decode_fmllr.sh: doing a final pass of acoustic rescoring.
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri4a/graph exp/tri4a/decode_test
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 35.2601156069% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 32.5581395349% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri4a/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,3) and mean=2.0
steps/diagnostic/analyze_lats.sh: see stats in exp/tri4a/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri4a/graph exp/tri4a/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
Pass 5
Pass 6
Estimating fMLLR transforms
Pass 7
Pass 8
Pass 9
Pass 10
Aligning data
Pass 11
Pass 12
Estimating fMLLR transforms
Pass 13
Pass 14
Pass 15
Pass 16
Pass 17
Pass 18
Pass 19
Pass 20
Aligning data
Pass 21
Pass 22
Pass 23
Pass 24
Pass 25
Pass 26
Pass 27
Pass 28
Pass 29
Pass 30
Aligning data
Pass 31
Pass 32
Pass 33
Pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri5a
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 36.5043534344% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 24.376012966% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri5a/log/analyze_alignments.log
8986 warnings in exp/tri5a/log/fmllr.*.*.log
1 warnings in exp/tri5a/log/compile_questions.log
2 warnings in exp/tri5a/log/analyze_alignments.log
1 warnings in exp/tri5a/log/build_tree.log
157 warnings in exp/tri5a/log/align.*.*.log
1 warnings in exp/tri5a/log/questions.log
169 warnings in exp/tri5a/log/init_model.log
612 warnings in exp/tri5a/log/acc.*.*.log
30 warnings in exp/tri5a/log/est_alimdl.log
4053 warnings in exp/tri5a/log/update.*.log
steps/train_sat.sh: Likelihood evolution:
-52.501 -52.1059 -52.0406 -51.7518 -50.501 -48.7818 -47.5432 -46.6511 -45.8949 -45.179 -44.5826 -43.5862 -42.9613 -42.5344 -42.1384 -41.7573 -41.3946 -41.0459 -40.7114 -40.3726 -40.0658 -39.8259 -39.599 -39.3981 -39.2104 -39.0443 -38.9258 -38.8403 -38.7839 -38.7406 -38.6936 -38.6716 -38.6503 -38.6352 
exp/tri5a: nj=20 align prob=-41.53 over 4.18h [retry=1.1%, fail=0.6%] states=2720 gauss=74096 fmllr-impr=1.36 over 3.54h tree-impr=11.34
steps/train_sat.sh: done training SAT system in exp/tri5a
steps/align_fmllr.sh --cmd run.pl --nj 20 data/train data/lang exp/tri5a exp/tri5a_ali
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train using exp/tri5a/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri5a_ali
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 36.3431151242% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 24.376012966% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri5a_ali/log/analyze_alignments.log
50 warnings in exp/tri5a_ali/log/align_pass2.*.log
2 warnings in exp/tri5a_ali/log/analyze_alignments.log
52 warnings in exp/tri5a_ali/log/align_pass1.*.log
2244 warnings in exp/tri5a_ali/log/fmllr.*.log
./run.sh: train chain model
local/chain/run_tdnn.sh 
tree-info exp/tri5a/tree 
local/nnet3/run_ivector_common.sh: preparing directory for low-resolution speed-perturbed data (for alignment)
utils/data/perturb_data_dir_speed_3way.sh: data/train_sp/feats.scp already exists: refusing to run this (please delete data/train_sp/feats.scp if you want this to run)
tree-info exp/tri5a/tree 
make-h-transducer --disambig-syms-out=exp/tri5a/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_3_1 exp/tri5a/tree exp/tri5a/final.mdl 
fsttablecompose exp/tri5a/graph/Ha.fst data/lang_test/tmp/CLG_3_1.fst 
fstrmepslocal 
fstrmsymbols exp/tri5a/graph/disambig_tid.int 
fstminimizeencoded 
fstdeterminizestar --use-log=true 
fstisstochastic exp/tri5a/graph/HCLGa.fst 
0.000486833 -0.101795
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri5a/final.mdl exp/tri5a/graph/HCLGa.fst 
steps/decode_fmllr.sh --cmd run.pl --nj 20 --config conf/decode.config exp/tri5a/graph data/test exp/tri5a/decode_test
steps/decode.sh --scoring-opts  --num-threads 1 --skip-scoring false --acwt 0.083333 --nj 20 --cmd run.pl --beam 8.0 --model exp/tri5a/final.alimdl --max-active 2000 exp/tri5a/graph data/test exp/tri5a/decode_test.si
decode.sh: feature type is lda
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri5a/graph exp/tri5a/decode_test.si
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 37.5722543353% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 44.7674418605% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test.si/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,4) and mean=2.1
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test.si/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/decode_fmllr.sh: feature type is lda
steps/decode_fmllr.sh: getting first-pass fMLLR transforms.
steps/decode_fmllr.sh: doing main lattice generation phase
steps/decode_fmllr.sh: estimating fMLLR transforms a second time.
steps/decode_fmllr.sh: doing a final pass of acoustic rescoring.
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri5a/graph exp/tri5a/decode_test
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 38.4393063584% of the time at utterance begin.  This may not be optimal.
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 45.3488372093% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,3) and mean=2.0
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
